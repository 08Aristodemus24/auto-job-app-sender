# Path to data engineering starts at the bottom of the barrel: 
Any kind of analyst. From there you can pivot to a regular data analyst role then to a senior data analyst then to a data engineer 

Most data teams in 2023 are shrinking, and I think the trend will continue in 2024, mainly because of AI. That makes it difficult to get a junior role as a data analyst because the bar is constantly being raised. Business users leveraging AI today are equivalent to data analysts from 5 years ago, albeit lacking some skills such as SQL.

  
One way to approach this challenge is to target roles containing the term 'analyst,' even if 'data' is not explicitly mentioned, and then apply for a data analyst position. However, if I were to start anew, I would focus on utilizing AI for data analysis and mastering storytelling, rather than personally delving deep into data analysis.
  

  
Sales Operations can be a decent entryway. Working with largely transactional data, light sql and heavy spreadsheet work. At least from my experience. Depends what your end-goal is.

Hey buddy. Basically any job that has access to Excel or SQL can help you transition into a data analyst role. Before I got my first job in tech I was working as an administrative assistant for a property management company and they often needed custom reports off of their crystal reports server. Using this tool allowed me to learn the foundation of SQL Syntax (using FILTERS predicates simple joins etc). From there I transitioned from using a GUI to make the queries into writing them direct in SQL myself.

Def use a course online or book to learn SQL. There is so many available but I personally like eDx since it's free but the courses are taught by Harvard and MIT professors.

Logistics Analyst

Import/Export Analyst

Supply Chain Analyst

I'm trying to transition from Logistics over to Data Viz. & Analysis, so I'm leveraging my experience & knowledge in this industry to jump over to the other side.

In my current industry, excel is really all you need for an Analyst position, but if you can incorporate things like Power BI or Tableau in to this, you can start creating a portfolio.

# Media Meter
interviewer descrbes workflow and technologies
- web scraping
- data orchestration
- ml they use pub/sub I can relate with this
- I currently had experience with RAG agetns at my current internship
- salina is cool, this ml can take in fiels and interpet it
transcription using whisper X from audio/video, speech to text AI, can chapterize and summarize submitted video/audio

they'll list the technologies they use, some of it you have experience and some of it you won't list those as your strengths and weaknesses
strengths:
python
web scraping
node.js
in db management usually download mo yung management system like mongo db compass for mongo db, pgadmin 4 for postgresql, or mysql workbench for mysql, add to path the bin folder for accessible commands in command line, create instance in cloud or management system application itself and then save the insances credentials this will be the URI you will need with hostname and password you will need to connect to the instance, then we connect to the db via this instance

weaknesses:
redis 
etl
apache airflow
aws
azure
docker/containerization
separate my code based on functionality like django with apps, urls, settings
basically I should've separated the UI from the insertion of the reading of data and insertion of said data to db
use try catch block for potential errors in database transactions
try to write utilities.py that contains all helper functions
replace index.py with a more udnerstandable one where the name of the file represents what happens in the source code itself
naubusan lang ako oras pero important rin may comments, lagyan ng type ang mga arguments

employer will describe their worflow:
automatic web scraper, SQL, 
orchestrate
transform data
pass transformed data to ml model (salina) to train
proof of concept (venn diagram) -> prototype code -> execution

## Final interview might consist of the common...

### Why do you want to work here?
I think the idea of collecting and operating on large swathes of data
and using data from news articles, technological trends, medical data to allow other clients
to make their own decisions based on the data we could process for them really caught my eye
because in all honesty I'm just someone who's fond of doing so and i think doing it on the job will be a
great learning experience 

### Why should we hire you?
In all humility apart from obvious skills, I draw from my personal experience of quite frankly being always a lone developer
if there is such a term, and if I'm given the opportunity here to collaborate with a team of like minded individuals someone as wanting to grow just as much as me, and wanting to contribute just as much as me, I'll be driven to do my best to give back to this organization by doing my due diligence to always committing to the tasks at hand 

### Describing their workflow:
* ung meron naman impediments like yung siguro nahihirapan sa tasks ganun kilangan iresolve kasi every week dapat may milestone na natatapos
* they use agile methodology to develop applications, scrum to be specific. 2 - 4 weeks sprints. After these 2 or 4 weeks may natatapos na na feature ng application or nakapagpush na sa dev then into production branch. As a result some tools you really need to learn are docker for containerization, pulling from development branch and then merging in this branch
* product backlog - list of tasks for all sprints
* sprint backlog - specific tasks in the product backlog for a specific sprint lasting 1 to 4 weeks. E.g. for week 1 these are the tasks
* after completing tasks in the sprint after say 1 week there will be a presentation of your work to your colleagues



# GCash
* find malicious users
* gscore
* recommender systems, had cold start problems too much new items and new users 
* generative ai chatbot for welath advice for
* for llms azure enteerprise are the ones they use
* internship nalang
* thomson sampling online reinforcement algorithm
* add inigo miguel benavides on linked in
* they use AWS sagemaker for ML deployment, 


# Xurpas Enterprise, Inc.
* what are 3 main you'd want to resign. Work culture, or if meron toxicity in the workplace, micro management
* what is python?
* when is try except used?
* tell me about yourself. The answer here is not describing what your hobbies are, not your age, gender, or location, but your work experience an example can be the ff.

I was/am a data engineer intern at my previous company virtuals protocol where I've mainly used python as a programming language and libraries like pandas, numpy, tesseract, to clean and preprocess datasets for RAG AI agents or retrieval augmented generation AI agents. These preprocessed datasets coming from raw ones like pdfs, images, spreadsheets, jsons, word documents are then ingested by the AI agent to fine tune its knowledge base. This helps increase its market capital as an agent operating in environments like twitter in a purely automated fashion to engage users and have those users invest in it thus increasing capital.

my background is computer science where I've been for the last 6 years of my study learning all that I can about machine learning and deep learning and artificial intelligence using primarily python as a language and vast frameworks made for ML/AI like tensorflow, pytorch, numpy, pandas to build many of the projects I have revolving around AI/ML and more recently Data Engineering/ETL pipelines for seamless AI/ML model training.

[describe one of your personal projects or more accurately your dissertation which focuses on identifying noise/interferences in electrodermal activity signals recorded from human beings, this is to remove eventually these interferences and be able to use the cleaned signals for various medical tasks like stress detection or arousal detection]

This was the main specialization I chose given not only the rising trend with it but also by virtue of it capturing my interest because I thought AI could be healthcare one of my main shall we say advocacies that I'd like to partake in in the future/

in between my junior and senior year and even after I built more of the aforementioned personal projects and sought internships that honed my skills further in data preprocessing, data cleaning. One of my colleagues who I'd consider a prominent figure in the Philippine AI/ML tech space is theo ne that recruited me to the data engineerign role I interned for (Armielyn Obinguar).

* what do you do to manage stress?
I go outside, exercise and just try not to think about my work for a menial 1 hour or 45 minutes. Being in the sun, talking with other people, does wonders for keeping my mental health in check.

* why could you contribute to the team?
given the vast knowledge I have and had collected in all my 6 years as an undergrad particularly with regards to using AI/ML technolgoies I think I can share this knowledge I have to my fellow coworkers, but apart from this I like to think of myself as collaboratvie and encouraging in that if I see my fellow coworker struggling with their task I'd want to guide them through it and sit there with them to see perhaps the error through and share what i can in order for them to solve their problem at hand.  


# Datung
* solving credit problem in small businesses to increase capital of small business to grwo more as a business
* other banks don't lend capital or loans to small businesses and datung solves that
* work with small businesses like cash only business. Manuyal ledgers nila and they need to find a way to verify it, using computer vision.
* see if the client is a good payer
* this is interesting kasi were using computer vision to determine whether a clients manually encoded ledger is someone worthy ot be lent a loan.
* An accounting ledger is basically an account or record used to store bookkeeping entries for balance-sheet and income-statement transactions. Accounting ledger journal entries can include accounts like cash, accounts receivable, investments, inventory, accounts payable, accrued expenses, and customer deposits. So it can be determined if a customer is trustworthy not only worthy of being lent a loan is if nagbabayad naman siya on time sa mga utang niya, if madalas nga ba siya magbayad on time, etc.
maganda to kaso sa paranaque nga lang
* maaayos yung CEO, they work as a team, close knit, marami lang responsibilities. I'm thankful for the assurance that it's okay to ask for help because I will not know what I'm doing I will not know the outcome, but I'm willing to fail trying.
* Automated Credit Scoring
- Machine learning models for credit risk assessment
- Alternative data analysis
- Real-time scoring capabilities
* Visual Assessment Tools
- Computer vision for business verification
- Automated document processing
- Visual inventory analysis
* Risk Management Systems
- Predictive risk modeling
- Portfolio monitoring
- Early warning systems
* Loan Management Platform
- End-to-end loan processing
- Automated underwriting
- Collection management
* magkakaroon naman ng office in 1 year time, and they expect me to stay for the long term. Kung okay naman ang pay and naggrow ang experience in various technologies then its a good offer for me. Their website upon first glance tells me their good at what they do.


How can I leverage an individuals manually encoded accounting ledger that details their businesses cash flow, how much they are making, losing, as a dataset that tells me whether they are worthy of being lent a loan? What variables can be possibly used for a predictive model to use to predict whether form this individuals ledger can he be worthy of borrowing a loan?



Leveraging a manually encoded accounting ledger for loanworthiness assessment requires careful extraction and transformation of data into usable variables. Here's a breakdown of how you can approach this, along with potential variables for a predictive model:

1. Data Extraction and Cleaning:

Digitization:
If the ledger is handwritten, you'll need to digitize it. This could involve manual data entry, OCR (Optical Character Recognition) if the handwriting is clear, or a combination of both.
Ensure accuracy during digitization. Double-check entries, especially numerical values.
Data Structuring:
Transform the ledger into a structured format like a CSV or Excel file.
Standardize date formats, currency symbols, and descriptions.
Categorize transactions into meaningful categories (e.g., sales revenue, expenses, loan repayments, etc.).
Data Cleaning:
Handle missing values. You might impute them based on averages or trends, or exclude records with excessive missing data.
Identify and correct outliers. These could be errors or genuine, but unusual, transactions.
Ensure consistency in transaction descriptions.
2. Variable Creation:

Here are potential variables you can derive from the ledger for a predictive model:

Cash Flow Related:
Average Monthly Cash Flow: Calculate the average net cash flow (inflows minus outflows) over a specific period (e.g., 6 months, 1 year).
Cash Flow Volatility: Measure the standard deviation or variance of monthly cash flow to assess stability.
Cash Flow Trends: Identify upward or downward trends in cash flow over time.
Days Cash on Hand: calculate how many days the business can operate with its current cash on hand.
Frequency of negative cash flow: How often does the business run out of money.
Revenue and Expense Related:
Average Monthly Revenue: Calculate the average monthly income.
Average Monthly Expenses: Calculate the average monthly expenditures.
Profit Margin: Calculate the profit margin (net profit/revenue).
Expense Ratio: Calculate the ratio of expenses to revenue.
Revenue growth rate: percentage of growth in revenue over a given period.
Debt and Liabilities:
Existing Debt: If the ledger includes loan repayments, you can infer existing debt levels.
Debt-to-Income Ratio: Calculate the ratio of debt repayments to income.
Frequency of loan or debt payments.
Operational Metrics:
Transaction Frequency: The number of transactions per month can indicate business activity.
Customer Concentration: If the ledger reveals customer details, analyze the distribution of revenue across customers. High concentration might indicate risk.
Supplier Concentration: Similar to customer concentration, high dependence on a single supplier can be a risk.
Time-Based Variables:
Business Operating History: The length of time the business has been operating.
Seasonality: Identify seasonal patterns in cash flow or revenue.
3. Predictive Modeling:

Model Selection:
Consider logistic regression, decision trees, random forests, or gradient boosting models.
The choice depends on the size of the dataset and the complexity of the relationships between variables.
Feature Engineering:
Combine or transform existing variables to create new, more informative features.
Use techniques like polynomial features or interaction terms.
Model Training and Evaluation:
Split the data into training and testing sets.
Train the model on the training set.
Evaluate the model's performance on the testing set using metrics like accuracy, precision, recall, F1-score, and AUC-ROC.   
Risk Scoring: create a risk score based upon the output of the model.
4. Considerations:

Data Quality: The accuracy of the model heavily relies on the quality of the ledger data.
Subjectivity: Manual encoding can introduce subjectivity.
Data Limitations: A single ledger might not capture all aspects of the business's financial health.
Legal and Ethical Issues: Ensure compliance with data privacy regulations.
Contextual Understanding: It is very important to try and understand the context of the business. For example, large purchases of equipment would be large outflows, but are not necessarily a sign of a bad business.
Verification: Whenever possible, verify information in the ledger with external sources (e.g., bank statements, invoices).
By following these steps, you can effectively leverage an individual's manually encoded accounting ledger to build a predictive model for loanworthiness assessment.

can you give me a sample ledger and sample variables you can engineer or use for a predictive model?

```
Date,Description,Category,Inflow,Outflow,Balance
2023-10-01,Starting Balance,Opening Balance,0,0,1000.00
2023-10-02,Sales - Widget A,Sales,500.00,0,1500.00
2023-10-03,Rent Payment,Expenses,0,300.00,1200.00
2023-10-05,Supplies Purchase,Expenses,0,150.00,1050.00
2023-10-08,Sales - Widget B,Sales,300.00,0,1350.00
2023-10-10,Marketing Expenses,Expenses,0,100.00,1250.00
2023-10-15,Sales - Widget A,Sales,400.00,0,1650.00
2023-10-18,Utilities Payment,Expenses,0,80.00,1570.00
2023-10-22,Loan Repayment,Debt,0,200.00,1370.00
2023-10-25,Sales - Widget C,Sales,600.00,0,1970.00
2023-10-28,Owner's Draw,Personal,0,250.00,1720.00
2023-10-31,Ending Balance,Closing Balance,0,0,1720.00
2023-11-01,Starting Balance,Opening Balance,0,0,1720.00
2023-11-03,Sales - Widget A,Sales,550.00,0,2270.00
2023-11-06,Rent Payment,Expenses,0,300.00,1970.00
2023-11-09,Supplies Purchase,Expenses,0,180.00,1790.00
2023-11-12,Sales - Widget B,Sales,350.00,0,2140.00
2023-11-15,Marketing Expenses,Expenses,0,120.00,2020.00
2023-11-19,Sales - Widget A,Sales,450.00,0,2470.00
2023-11-23,Utilities Payment,Expenses,0,90.00,2380.00
2023-11-26,Loan Repayment,Debt,0,200.00,2180.00
2023-11-29,Sales - Widget C,Sales,650.00,0,2830.00
2023-11-30,Owner's Draw,Personal,0,250.00,2580.00
2023-11-30,Ending Balance,Closing Balance,0,0,2580.00
Engineered Variables:
```

Based on this sample ledger, here are examples of variables you can engineer for your predictive model:

1. Cash Flow Variables:

Average Monthly Net Cash Flow:
Calculate the net cash flow (Inflow - Outflow) for each month.
Average these monthly net cash flows.
Example: (720 + 860) / 2 = 790.
Monthly Cash Flow Volatility:
Calculate the standard deviation of monthly net cash flows.
This measures the fluctuation in cash flow.
Number of Negative Cash Flow Months:
Count the number of months where net cash flow is negative.
Minimum Monthly Balance:
Find the lowest balance that occured within the month.
Days Cash on Hand (estimated):
Divide the current balance by the average daily outflow.
Cash Flow Trend:
Calculate the change in average cash flow over time (e.g., compare the last 3 months to the previous 3 months).
2. Revenue and Expense Variables:

Average Monthly Revenue:
Sum the "Inflow" values for each month and calculate the average.
Example: (1700 + 2000) / 2 = 1850.
Average Monthly Expenses:
Sum the "Outflow" values (excluding "Personal" and "Opening/Closing Balance") for each month and calculate the average.
Example: (980 + 1140) / 2 = 1060.
Profit Margin (Monthly):
(Monthly Revenue - Monthly Expenses) / Monthly Revenue.
Expense Ratio (Monthly):
Monthly Expenses / Monthly Revenue.
Sales Variability:
Standard deviation of monthly sales revenue.
Expense Variability:
Standard deviation of monthly expenses.
3. Debt and Liabilities Variables:

Average Monthly Loan Repayment:
Sum the "Outflow" values where "Category" is "Debt" and calculate the average.
Debt-to-Cash Flow Ratio:
Average Monthly Loan Repayment / Average Monthly Net Cash Flow.
Debt Payment Frequency:
How often debt payments are made.
4. Operational Variables:

Transaction Frequency (Monthly):
Count the number of transactions per month.
Owner's Draw Frequency and Amount:
Frequency and average amount of withdrawals.
5. Time-Based Variables:

Months of Operation:
Calculate the number of months the ledger covers.
Seasonality Indicators:
If you have more data, you could identify seasonal patterns in sales or expenses.
How to Use the Variables:

Create a table: Organize the calculated variables into a table, with each row representing a month or a summary of the ledger.
Add target variable: Add a column representing the target variable (e.g., "Loan Default" - 1 for default, 0 for no default). This will need to come from external data, such as a history of loan payments.
Train your model: Use the engineered variables as features to train your predictive model.
Important Notes:

The quality of the ledger data is crucial.
These are just examples; you can engineer more variables based on the specific needs of your model.
Always consider the context of the business when analyzing the data.
External data is important. The ledger alone will not provide a target variable for training a predictive model.

To whom it may concern


{position} {company_name}

We strive to provide sound technical consultancy and implementation services for complex IT systems while building a company that respects its employees.

To achieve our mission, we strive:

To position ourselves as a cost-effective market leader facilitating our customers to be competitive
To understand our customers’ needs and challenges precisely, so as to align our solutions to meet their needs
To respect our employees by believing in their creative talents, harnessing their sense of commitment and entrepreneurial flair, while providing an excellent environment for people to work in
To leverage technology and industry knowledge in innovating best-in-breed solutions for our customers

Commitment to the highest quality
We are committed to our clients in delivering quality services that have so far won their confidence.
Responsibility and Integrity
We believe that it is very important to demonstrate professionalism in all of our client engagements, especially to maintain open and honest communication in delivering services.
Collaboration and Teamwork
We encourage and promote an environment of teamwork and collaboration among our people so as to grow and share the success of Azeus.

# Azeus Systems LTd.
## Behavioral Questions:
* what qualities are you trying to find in a company? `open honest communication, mutual support, responsibility and integrity `

* what are your 3 biggest strengths and explain each one? 
- `meticulousness: as I always make sure to analyze the quality of the code I write and make sure most if not every case is taken into consideration`
- `creativity: I really like to implement designs in code that elicit beauty and inspiration`
- `collaboration: I like to share my thoughts of approaching a problem with my teammates and asking in turn their opinion on how they would solve it, and sharing our solutions right after and come to a reasonable way how to integrate it`

* what are your weakness as a person and as a worker/student and how did you confront them? `sometimes I tend to keep problems (both technical and non-technical) to myself, and while independence has its merits it means that I lead myself at times to unnecessary amounts stress, so I always make sure to understand the problem clearly by asking my colleagues and then ask without shame help if needed`

* what are you trying to avoid when working (they give work life balance kaya answer you don't want to burn out well not explicitly but as much as possible you try to avoid enough work and then balance with it other endeavors)? `I know I'm a young developer I have much to learn and naturally that may come out in trying to take on as much responsibility as physically possible as a way to learn and prove myself, so I avoid biting off more than I can chew because I've learned that everything can just be taken one step at a time as opposed to all at once at the cost of being burnt out in long run`

* how do you manage your time and schedule? `I find that in the context of work I write down the task what this task requires what other requirements those requirements require, and see the simplest task/requirement I can accomplish this way I manage my time by not procrastinating through being overwhelmed at the task given. I make sure to also do my routine like eating, sleeping, taking little breaks here and there in an orderly manner so as to keep my schedule and time on track.`

* were there times that you had to choose schoolworks over personal events? ``

* What were the results of your 3 strengths in thesis? 
- `meticulousness: I made sure the signal features we had were properly extracted and through the functions we wrote, we made sure to understand the dataset fully in order to implement the features needed for our system to train on`
- `creativity: the user interface is something I'm most proud of because of its ability to be simple but also functional and informative, the design is also something I'm most proud of as it elicits the paradigm of minimalism in something as complex a system as ours`
- `collaboration: me and my group mates had our strengths in doing other aspects of our thesis, but one aspect was really challenging in that we had no adequate resources to train our system using our large dataset. I offered solutions to avail computing resources from providers like google to my groupmates but with mutual effort in researching other means my other groupmate offered to instead avail computing resources from DOST which was luckily completely free of charge, and luckily could be used using a Linux OS which I was already familiar with, allowing us to train our system without much hassle from cost and most especially not having a large learning curve to use the resource`

* what are the programming languages you're familiar and proficient in? `Python, JavaScript, SQL, C++`

* what to do when having conflicting ideas with other members? 
```
1. know the idea I have in mind and want implemented 
2. know the idea my team member personally wants to implement
3. assess the strengths and weaknesses of each idea
4. it is at times unavoidable that he/she will have the better solution and so sometimes you have to put your ego aside but sometimes I reasonable integration of both ideas strengths can be in fact better as it not only maybe a better solution but both parties can be mutually satisified. (You can both have your cake and eat it too so to speak)
```

* what's your salary expectation
`I've researched and seen the typical salary range of mid level software developers and engineers in this company amid my application and the range is between the lower range of 250k/year to 500k/year or 20800/month to 41700/month, so the number I'm comfortable with would be 30000/month based on my current knowledge and expertise does that fit into the company budget? But I'd like to learn more about the specific technologies tailored for this position as that will determine further my expectations for my salary.`

* what kind of personality you like to work with?
* what are the types of people you don't want to work with?


* tell me about yourself:  
```
I was a data engineer intern at virtuals protocol where 
* I mostly cleaned, preprocessed mostly stock data for fine tuning Retrieval Augmented Generation AI agents the technologies 
* I used here was mainly numpy, pandas, specific frameworks for extracting ifnormation from files such as word documents and pdf files
* I also worked on developing shell scripts to automate the ingestion process of the ai agent of a client of the datasets we processed
* I also worked on writing relevant guides for clients on how to write their own 
```
- `I'm a recent graduate of Computer Science in the Polytechnic University of the Philippines where I picked up in part most of my skills in software development & data science and now more personally to data analytics`
- `in my time during and after college I made personal projects mostly revolving around machine learning integrated with fullstack web frameworks learning a ton about how typical machine learning projects work and how each step like preprocessing, deploymnet, and testing works under the hood.`
- `outside of work I've really been fond of creating personal projects more geared in data analytics and using tools like python, SQL, powerbi to transform data and visualize it using dashboards`
- `all in all I'm just really looking for an opportunity to use the skills I have now in data science and full stakc development and translate it to the workplace where I can continually grow`

* why should we hire you (more selfish reasons)?
apart from the technical skills

* why do you want to work here?
`Having first learned about your company 2 years ago what I really value is that your compnay strives for the virtues of responsibility, teamwork, and honesty, which resonate with me because I believe I can be responsible enough to own my shortcomings, support my teammates by listening to their concerns and questions, and be completely honest about certain predicaments I may be in, or concerns I have without fear of ridicule or shame. I think these are essential in the workplace which funnily enough can be translated into human to human relationships themselves be it platonic or romantic.`

`I think the company fosters growth in technologies which I'm greatly enthusiastic about, because while monetary value is important we cannot deny this, learning new things is just as equally important and I think learning new things like technologies can greatly contribute to developing quality products. On a more simple note learning new things is fun for me personally and learning new things through hands on building is what I'm fond of doing the most`


* learn to defend your code if your teammate does not agree with how you wrote it e.g. using performant code over readable code in some cases
* dont work in silence, communicate if may mali sa data niyo to data engineers, have someone quality assure your code

* How do you make sure that you deploy quality code in
production?
* What would you do if you deploy bad code in prod?
* What would you do if you have duplicates in your data in prod?
* If your teammate does not agree with your code, how do you handle that situation?

## Technical Coding Interview Questions:
### DSA Related:
1. FizzBuzz variation question from Leetcode 
2. Simple mathematics and logic question 
3. Maximum subarray sum from Leetcode (uses kadanes algorithm) `done practicing`
4. Print an hourglass (according to their hourglass pattern) `done practicing`
5. Given two arrays, one which is rotated (either left or right), find the least times of rotations of the un-rotated array to match the rotated one. (gawin mo dito ay compare the newly rotated array each time to the old rotated array, and count the number of times you compared the array to get the two to be equal to each other)
6. Last unique character
7. Check if array is sorted or not

### SQL Related
1. Identify which rows are duplicated
```
with duplicates as (
    select emp_id, row_number() over(partition by emp_id, emp_name, emp_salary order by emp_id) as row_occ
    from employee
)

select emp_id, max(row_occ)
from duplicates
group by emp_id
```

2. remove duplicate values
```
select *
from employee
group by emp_id, emp_name, emp_salary
```

3. check if all values in a table have the same values in another table
```
select id from table_a
minus
select id from table_b
union all
select id from table_b
minus
select id from table_a
```

4. find the second-highest salary per department, but exclude departments where fewer than 3 employees exist.
```
CREATE TEMPORARY TABLE IF NOT EXISTS temp (
    emp_id INTEGER PRIMARY KEY,
    emp_name VARCHAR,
    emp_salary INTEGER,
    dept_id INTEGER NOT NULL
);

INSERT INTO temp (emp_id, emp_name, emp_salary, dept_id) 
VALUES
(1, 'john', 50000, 1),
(2, 'wayne', 50000, 1),
(3, 'doe', 75000, 1),
(4, 'chris', 50000, 2),
(5, 'leo', 80000, 2),
(6, 'david', 50000, 3),
(7, 'martin', 60000, 3),
```

```
| emp_id | emp_name | emp_salary | dept_id |
|    1   |   john   |    50000   |    1    |
|    2   |   wayne  |    50000   |    1    |
|    3   |    doe   |    75000   |    1    |
|    4   |   chris  |    50000   |    2    |
|    5   |    leo   |    80000   |    2    |
|    6   |   david  |    50000   |    3    |
|    7   |  martin  |    60000   |    3    |
```
since we are trying to find the 2nd highest employee salary we can't simply use group bys, to find the max. We can use a `dense_rank()` to assign ranks to each salary according to or partitioned by their `dept_id`. For instance we assign a rank of 1 to the hgihest salary lets say in the tech department, we assign a rank of 2 to the 2nd highest salary in the tech department, and so on. We need to partition on the `dept_id` and then order by the employee salary in descending order so that the hghest salary is always 1st the 2nd highest 2nd, the 3rd highest 3rd and so on.
```
SELECT *, DENSE_RANK() OVER(PARTITION BY dept_id ORDER BY emp_salary DESC)
FROM temp
```

```
emp_id	emp_name	emp_salary	dept_id	dense_rank() OVER (PARTITION BY dept_id ORDER BY emp_salary DESC)
7	martin	60000	3	1
6	david	50000	3	2
3	doe	75000	1	1
1	john	50000	1	2
2	wayne	50000	1	2
5	leo	80000	2	1
4	chris	50000	2	2
```

because we are also trying to exclude finding employees with the 2nd highest salary in a department where fewer than 3 (< 3) employees exist we can use also a `COUNT()` as a window function and assign counts each row according to or partitioned by each rows department. Kind of like using `SUM()` as a window function where instead of aggregating we calculate the sum of the values along the unique values of a column in this case the `dept_id` but still retain the number of rows.
```
SELECT *, 
  DENSE_RANK() OVER(PARTITION BY dept_id ORDER BY emp_salary DESC), 
  COUNT() OVER(PARTITION BY dept_id)
FROM temp
```

```
emp_id	emp_name	emp_salary	dept_id	dense_rank() OVER (PARTITION BY dept_id ORDER BY emp_salary DESC)	count() OVER (PARTITION BY dept_id)
7	martin	60000	3	1	2
6	david	50000	3	2	2
3	doe	75000	1	1	3
1	john	50000	1	2	3
2	wayne	50000	1	2	3
5	leo	80000	2	1	2
4	chris	50000	2	2	2
```

from here we can use a simple filter to get only the rows with >= 3 employees and a row with rank of 2 
```
WITH ranked_employees AS (
  SELECT *, 
    DENSE_RANK() OVER(PARTITION BY dept_id ORDER BY emp_salary DESC) AS emp_rank, 
    COUNT() OVER(PARTITION BY dept_id) AS dept_cnt
  FROM temp
)

SELECT *
FROM ranked_employees
WHERE emp_rank = 2 AND dept_cnt >= 3
```

or by using subquery (because window functions ussed with where cannot be used as it will raise an error)

```
SELECT * 
FROM (
    SELECT *, 
        DENSE_RANK() OVER(PARTITION BY dept_id ORDER BY emp_salary DESC) AS emp_rank, 
        COUNT() OVER(PARTITION BY dept_id) AS dept_cnt
    FROM temp
) WHERE emp_rank = 2 AND dept_cnt >= 3
```

5. For each user, calculate the 7-day moving average of transactions, considering transaction dates with gaps.
```
-- Create the transactions table
CREATE TABLE transactions (
    transaction_id INT PRIMARY KEY AUTO_INCREMENT,
    user_id INT NOT NULL,
    transaction_date DATE NOT NULL,
    amount DECIMAL(10, 2) NOT NULL
);

-- Insert sample data
INSERT INTO transactions (transaction_id, user_id, transaction_date, amount) VALUES
(1, 101, '2024-01-01', 10.50),
(2, 101, '2024-01-02', 15.00),
(3, 101, '2024-01-03', 20.00),
(4, 101, '2024-01-05', 12.75), -- Gap on Jan 4
(5, 101, '2024-01-06', 18.20),
(6, 101, '2024-01-07', 25.00),
(7, 101, '2024-01-08', 30.00),
(8, 101, '2024-01-09', 11.00),
(9, 101, '2024-01-12', 14.50), -- Another gap of 2 days since 2024-01-10 and 2024-01-11 have no transactions
(10, 101, '2024-01-13', 22.00),
(11, 101, '2024-01-14', 16.80),
(1, 102, '2024-01-01', 5.00),
(2, 102, '2024-01-03', 7.50),
(3, 102, '2024-01-04', 12.00),
(4, 102, '2024-01-08', 9.00),
(5, 102, '2024-01-09', 11.20),
(6, 102, '2024-01-10', 8.00);
```

because we have different users like 101 and 102, we will have to partition the table such that we calculate the 7-day moving average of a group of transactions of a specific user. Since the user will probably have days where they will not have transactions leaving dates where there are no transactions, this is of no obstacle to calculating the 7 day moving average. This would be a different matter however if we strictly needed to get the moving average of not only a users transactions but also their non transactions, if this was the case we would have to somehow impute the days without transactins or gap days with a value of 0 so that the avg() window function can pool instead the days with non transaction or values with 0 as opposed to pooling 7 days with purely only transactions. More on imputation on gap dates: 
- https://stackoverflow.com/questions/76203662/best-way-to-fill-in-gaps-between-dates-in-sql-table
- https://www.geeksforgeeks.org/mysql-lead-and-lag-function/

```
SELECT 
  *,
  AVG(amount) OVER(
    PARTITION BY user_id 
    ORDER BY transaction_date ASC 
    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
  ) AS ma_7
FROM transactions
```
Here we partitioned by each user. We also need to make sure the transaction date is ordered in ascending order to that our moving average is calculated according to this manner.

if we were however going to impute we would need to get the date difference in each subsequent transactin and count how many gap days there are inbetween each transaction.
```
WITH new_transactions AS (
  SELECT
    transaction_id,
    user_id,
    transaction_date AS curr_txn_date,
    LEAD(transaction_date) OVER(
      PARTITION BY user_id 
      ORDER BY transaction_date
    ) AS next_txn_date,
    amount
  FROM transactions
),

transactions_b AS (
  SELECT 
    *,
    -- we add a one because if a difference between a date is only
    -- 1 day then resulting value will be -1, we turn this to 0 by
    -- adding 1 to represent that there are no gaps in between the
    -- dates
    DATEDIFF('day', curr_txn_date, next_txn_date) - 1 AS date_diff
  FROM new_transactions
)

SELECT *
FROM transactions_b
```

resulting in table

```
transaction_id	user_id	curr_txn_date	next_txn_date	amount	date_diff
12	102	2024-01-01	2024-01-03	5.00	1
13	102	2024-01-03	2024-01-04	7.50	0
14	102	2024-01-04	2024-01-08	12.00	3
15	102	2024-01-08	2024-01-09	9.00	0
16	102	2024-01-09	2024-01-10	11.20	0
17	102	2024-01-10		8.00	
1	101	2024-01-01	2024-01-02	10.50	0
2	101	2024-01-02	2024-01-03	15.00	0
3	101	2024-01-03	2024-01-05	20.00	1
4	101	2024-01-05	2024-01-06	12.75	0
5	101	2024-01-06	2024-01-07	18.20	0
6	101	2024-01-07	2024-01-08	25.00	0
7	101	2024-01-08	2024-01-09	30.00	0
8	101	2024-01-09	2024-01-12	11.00	2
9	101	2024-01-12	2024-01-13	14.50	0
10	101	2024-01-13	2024-01-14	22.00	0
11	101	2024-01-14		16.80	
```


```
-- we generate a series starting from minimum date to
-- the maximum date as we will be using this temp generated
-- table to cross join on our transactions table
WITH date_lookup AS (
  SELECT * AS dt FROM GENERATE_SERIES( 
    (SELECT MIN(transaction_date)
    FROM transactions),
    (SELECT MAX(transaction_date)
    FROM transactions),
    INTERVAL 1 DAY
  )
),

user_max_min_txn_dates AS (
  SELECT 
    user_id, 
    MIN(transaction_date) AS min_txn_date, 
    MAX(transaction_date) AS max_txn_date
  FROM transactions
  GROUP BY user_id
),

imputed_dates AS (
  SELECT
    b.user_id,
    a.dt
  FROM date_lookup a
  LEFT JOIN user_max_min_txn_dates b
  ON a.dt BETWEEN b.min_txn_date AND b.max_txn_date
),

fixed_transactions AS (
  SELECT 
    b.transaction_id AS txn_id, 
    a.user_id, 
    a.dt AS txn_date, 
    COALESCE(b.amount, 0) AS amount,
  FROM imputed_dates a 
  LEFT JOIN transactions b
  ON a.user_id = b.user_id
  AND a.dt = b.transaction_date
  ORDER BY a.user_id, a.dt
)

SELECT 
  *, 
  AVG(amount) OVER(
    PARTITION BY user_id 
    ORDER BY txn_date ASC
    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
  ) AS amount_7ma
FROM fixed_transactions
```

6. Given two tables: projects(project_id, budget), employees(employee_id, project_id), find the project with the highest “budget per unique role.” Assume an employee_roles table exists.

7. Identify users who had a transaction amount 2x higher than their average transaction amount in the past 30 days.

8. Compare Import vs Direct Query modes in the context of handling real-time data from a distributed warehouse system with high latency.

### PowerBI Related
1. What is the difference between a visual-level filter, page-level filter, and a report-level filter? Provide an example where improper use leads to misleading analytics.

2. Design a Power BI dashboard with Row-Level Security (RLS) where a user might belong to multiple departments. How would you implement dynamic RLS using DAX? 

### Python Related
1. Using pandas, merge three datasets (sales, promotions, inventory). Return top 5 products with the highest uplift in sales during a valid promotion window.

2. Implement a custom function in pandas that bins continuous numerical data into deciles and labels outliers based on the IQR method. Return a summary table.|

3. Compare and contrast defaultdict, Counter, and regular dictionaries in Python. When would you use each for analyzing customer purchase patterns?

### Data Analytics related
* when would you use median vs mean in analyzing data?

* what is CAC? Customer Acquisition Cost. It is an indicator

* what is CLTV? Customer Lifetime Value

* what is MRR? Monthly Recurring Revenue

* what is Churn? te – % of users who stop using the product

* what is ARPU? Average Revenue Per User

* what is ROI? Return on Investment

* What is data analytics? Data analytics is the process of examining, cleaning, transforming, and modeling data to extract useful information, draw conclusions, and support decision-making.

* What are the types of data analytics? Descriptive, diagnostic, predictive, and prescriptive analysis.

* Explain the difference between qualitative and quantitative data? Qualitative data is non numerical, such as text or images, while quantitative data is numerical, such as measurements or counts.

* What is data cleansing? Data cleansing is the process of identifying and correcting errors, inconsistencies, and inaccuracies in datasets.

* What is data outlier? An outlier is a data point that significantly differs from the rest of the data points in a dataset.

* Explain the difference between SQL and NOSQL databases? SQL databases are relational, use structured query language, and have a predefined schema, while NoSQL databases are non-relational, use various query languages, and have a dynamic schema.

* What is ETL? ETL stands for Extract, Transform, and Load. It's a process for retrieving data from various sources, transforming it into a usable format, and loading it into a database or data warehouse.

* What is primary key in a database ? A primary key is a unique identifier for each record in table.

* What is foreign key in a database? A foreign key is a field in a table that refers to the primary key of another table, establishing a relationship between the two tables.

* Explain the difference between inner join and outer join in SQL? Inner join returns records with matching values in both tables, while outer join returns records from one table and the matching records from the other table, filing in NULL values for non-matching records.

* What is a histogram? A histogram is a graphical representation of the distribution of a dataset, showing the frequency of data points in specified intervals.

* What is a box plot? A box plot is a graphical representation of the distribution of a dataset, showing the median, quartiles, and possible outliers.

### Data Science/ML related
* What is linear regression? Linear regression is a statistical method used to modethe relationship between a dependent variable and one or more independent variables.

* What is overfitting? Overfitting occurs when a model is too complex and performs well.

* Explain the difference between Rsquared and adjusted R-squared? R-squared measures the proportion of variation the dependent variable explained by the independent variables, while adjusted R-squared adjusts for the number of independent variables in the model.

* What is a confusion matrix? In a confusion matrix is a table used to evaluate the performance of a classification model, showing the true positives, true negatives, false positives, and false negatives.

* What is K-means clustering? K-means clustering is an unsupervised machine learning algorithm used to partition data into k clusters based on their similarity.

* What is cross-validation? Cross-validation is a technique used to evaluate the performance of a model by splitting the dataset into training and testing sets multiple times and calculating the average performance.

* What is a decision tree? A decision tree is a flowchart-like structure used indecision making and machine learning, where each internalnode represents a feature, each branch represents adecision rule, and each leaf node represents an outcome.

* What is the difference between supervised and unsupervised learning? Supervised learning uses labeled data and a known output, while unsupervised learning uses unlabeled data and discovers patterns or structures in the data.

* Explain principal component analysis (PCA)?
ANS :- PCA is a dimensionality reduction technique that
transforms data into a new coordinate system, reducing
the number of dimensions while retaining as much
information as possible.
* What is time series analysis? Time series analysis is a statistical technique for
analyzing and forecasting data points collected over time,
such as stock prices or weather data.
* What is difference between a bar chart
and a pie chart? A bar chart represents data using rectangular bars,
showing the relationship between categories and values,
while a pie chart represents data as slices of a circle,
showing the relative proportion of each category.
* What is a pivot table? A pivot table is a data summarization tool that
allows users to reorganize, filter, and aggregate data in a
spreadsheet or database. 
* What is data normalization?
ANS :- Data normalization is the process of scaling and
transforming data to eliminate redundancy and improve
consistency, making it easier to compare and analyze.
* Explain the concept of data warehousing?
ANS :- A data warehouse is a large, centralized repository
of data used for reporting and analysis, combining data
from different sources and organizing it for efficient
querying and reporting.
* What is the role of a data analyst in a
company? A data analyst collects, processes, and analyzes
data to help organizations make informed decisions, identify
trends, and improve efficiency.
* How do you handle missing data in a
dataset? Missing data can be handled by imputing values
(mean, median, mode), deleting rows with missing data, or
using models that can handle missing data.
* How do you deal with outliers in a dataset? Outliers can be dealt with by deleting,
transforming, or replacing them, or by using models that
are less sensitive to outliers.
* Describe a situation where you used data
analysis to solve a problem? Answer this based on your personal experience,
detailing the problem, your approach, and the outcome.
* How do you ensure data quality and
accuracy in your analysis? Ensuring data quality and accuracy involves data
cleansing, validation, normalization, and cross-referencing
with other sources, as we I as using appropriate analytical
methods and tools. 
* Describe your experience with
programming languages, such as R or Python,
used in data analysis? Answer this based on your personal experience,
highlighting your proficiency
* How do you handle large datasets?
ANS :- Handling large datasets involves using efficient
data storage and processing techniques, such as SQL
databases, para lel computing, or cloud-based solutions,
and optimizing code and algorithms for performance.
* What is your experience with data
visualization tools, such as Tableau, Power Bl, or
Excel? Answer this based on your personal experience and
familiarity with the mentioned tools, providing examples of
projects or tasks you have completed using them
* How do you stay. upadated on the latest
trends and developments in data analysis? Mention resources such as blogs, podcasts, online
courses, conferences, and industry publications that you use
to stay informed and up-to-date.
* How do you handle data privacy and
security concerns in your analysis ?
* How do you ensure your data analysis is
unbiased? By being aware of potential biases, using diverse
data sources, applying objective analytical methods, and
cross-validating results with other sources or techniques.
* What metrics do you use to evaluate the
sucess of a data analysis project? Metrics may include accuracy, precision, reca I, F1
score, R-squared, or other relevant performance measures,
depending on the project's goals and objectives
* How do you determine the most
appropriate data analysis technique for a given
problem? By understanding the problem's context, the nature
of the data, the desired outcome, and the assumptions and
limitations of various techniques, selecting the most suitable
method through experimentation and validation.
* How do you validate the results of your
data analysis? By using cross-validation, holdout samples,
comparing results with known benchmarks, and checking
for consistency and reasonableness in the findings.


# MSCI
## Values:
* relentlessly innovative
* actively connect to collaborate
* always act with integrity

## Behavioral Questions:



# Xpress
## Values:
The organizational culture at XPRESS is dynamic, with a strong emphasis on collaboration and resource accessibility, which supports a productive work environment. However, the company faces challenges such as high turnover and a lack of remote work flexibility. Management practices often lean towards micromanagement, and there are noticeable gaps in cultural sensitivity, particularly regarding local customs and holidays. These issues can impact employee morale and the overall workplace atmosphere.



# Cebuana Lhuillier
* what is your expected salary? I've personally done some research on roles similar to the data analyst role in your company and  the salary range I'm comfortable in disclosing is around 20000/month to 25000/month, but in order to say the exact amount I will have to learn more about the tasks I will have to do on a daily basis and from there gauge my own performance and the value I give back as it will determine further a reasonable expectation for my salary
* Shift your mindset from i want to work at this prestigious company to instead i want to work at a company doesn't matter how prestigious and i want to be able to leave it at least 1% better

* do you have any more questions for us?
- "how's it like working there?"
- "I really learned alot from the company but I'm curious would there be some advice or feedback you could give for applicants such as myself on what I could improve more on should I potentially have a chance to work here?": work on being able to deliver your work experience in a concise manner e.g. saying what you did and what technologies you used to do whaht you did

# Accenture
* salary range for ASWE is 20000 to 25000 per month

# Inquiro
* https://inquiro.ph/location-data-and-footfall-analytics-insights/



# Plan
* eo basically even a junior data analyst role is so saturated with competition because its the only entry level data role. No one is hiring emtry level data engineers and machine learning engineer or ai developers if anyone gets hired for it out of college they are already the exceptions the superstars the ones who were lucky enough to learn data at a quick level and who had enough connections for someone to take them under their wing and evntualy get opportunities to train and then have that data role. Pero ikaw you're a statistic, you're just like anybody else na kumakayod, you're not special, you're not a quick learner, you don't have many connections for referrals. you are starting at the bottom of the barrel and you wont have any chamce at someone spoonfeeding you like those lucky bastards. You're going to have to start at the very bottom like a junior financial analyst, junior sales operations analyst, 
and then once in you can leverage your python, sql, powerbi skills to automate what is supposed to be a "manual" job, you can use your data skills to automate these things and thereby gain domain knowledge especially financial or sales or healthcare knowledge and domain knowledge combined with tools you've used that wasn't even oart of the job description in your resume when your finding your next job will be unbelievably powerful. You have to document these things though and this can be done by the ff.

That's a smart and proactive strategy! Many people gain valuable data analysis experience in roles that aren't explicitly titled "Data Analyst." The key is to effectively document and showcase these experiences in your CV/resume. Here's how you can do that:
1. Quantify Your Achievements Whenever Possible:
 * Numbers Speak Volumes: Instead of saying "Improved reporting," say "Automated weekly sales reports using Python and reduced reporting time by 40%."
 * Impact-Focused Language: Frame your accomplishments in terms of their impact on the business. For example, "Developed a Power BI dashboard to visualize key logistics KPIs, enabling a 15% reduction in delivery delays."
 * Use Metrics: Mention specific metrics like cost savings, efficiency gains, error reduction, increased revenue, or improved customer satisfaction.
2. Detail Your Data Analysis Tasks Under Each Relevant Job:
 * Go Beyond the Official Job Description: Don't just list your formal responsibilities. Think about the tasks you actually performed that involved data analysis, even if they weren't in your initial job description.
 * Use Action Verbs: Start each bullet point with strong action verbs that highlight your skills (e.g., Analyzed, Developed, Implemented, Automated, Visualized, Extracted, Transformed, Loaded, Queried, Identified, Predicted).
 * Be Specific: Instead of "Worked with data," be specific about the tools and techniques you used and the outcomes.
Example of Detailing Data Analysis Tasks:
Let's say your official job title was "Logistics Coordinator." You could describe your responsibilities like this:
Logistics Coordinator | ABC Company | [Dates of Employment]
 * Managed and tracked daily shipment schedules, ensuring timely deliveries and resolving logistical issues.
 * Leveraged SQL queries to extract and analyze shipment data from the company database to identify bottlenecks and inefficiencies in the delivery process.
 * Developed a Python script to automate the generation of daily delivery performance reports, reducing manual reporting time by 50%.
 * Designed and implemented interactive dashboards using Power BI to visualize key logistics KPIs (e.g., on-time delivery rate, transportation costs), enabling stakeholders to monitor performance and identify areas for improvement.
 * Conducted ad-hoc data analysis using Excel and Power BI to investigate specific logistical challenges and provide data-driven recommendations to management.
 * Collaborated with the warehouse and transportation teams to implement process improvements based on data insights, resulting in a 10% reduction in late deliveries.
3. Create a "Skills" Section that Highlights Your Data Analysis Proficiency:
 * List Technical Skills: Clearly list your proficiency in Python, SQL, Power BI (or other BI tools), Excel, and any other relevant software or programming languages.
 * Categorize Skills (Optional but Helpful): You can group your skills into categories like "Programming Languages," "Database Management," "Data Visualization," and "Statistical Analysis" for better readability.
4. Use a "Projects" or "Portfolio" Section (Highly Recommended):
 * Showcase Your Work: If you have personal projects or significant data analysis tasks from your previous roles that you can showcase (even in an anonymized or summarized way if company data is sensitive), create a dedicated section.
 * Describe the Problem, Your Approach, and the Outcome: For each project or task, briefly explain the problem you were trying to solve, the tools and techniques you used, and the results you achieved.
 * Link to a Portfolio (If Applicable): If you have a GitHub repository or a personal website showcasing your data analysis projects, include a link in your contact information.
5. Tailor Your CV/Resume to Each Job Application:
 * Match Keywords: Carefully review the job description and incorporate relevant keywords related to data analysis, the industry, and the specific tools mentioned.
 * Highlight Relevant Experiences: Emphasize the experiences and skills that are most relevant to the specific requirements of the job you are applying for.
6. Be Prepared to Elaborate in Interviews:
 * STAR Method: When discussing your experiences, use the STAR method (Situation, Task, Action, Result) to provide clear and concise examples of your data analysis work.
 * Be Confident: Be prepared to confidently explain how the tasks you performed in your previous roles demonstrate your data analysis skills and your ability to contribute to the new role.
Example of CV Snippets:
Skills:
 * Programming Languages: Python (Pandas, NumPy, Scikit-learn), SQL
 * Data Visualization: Power BI, Tableau, Matplotlib, Seaborn
 * Database Management: MySQL, PostgreSQL
 * Tools & Technologies: Excel (Advanced), Git, Cloud Platforms (e.g., AWS, Azure - if applicable)
 * Analytical Skills: Data Mining, Statistical Analysis, Data Cleaning, Data Wrangling, Exploratory Data Analysis (EDA)
Experience:
[Previous Job Title] | [Company Name] | [Dates of Employment]
 * [Regular job duties]
 * Conducted in-depth analysis of [specific dataset related to finance/logistics] using SQL to identify trends and anomalies, leading to [quantifiable result].
 * Automated the generation of [specific report] using Python, reducing the monthly reporting cycle by [percentage].
 * Developed interactive dashboards in Power BI to track [key performance indicators], providing stakeholders with real-time insights into [area of impact].
By meticulously documenting your data-related tasks, quantifying your achievements, and highlighting your skills, you can effectively demonstrate your data analysis capabilities in your CV/resume, even if your previous job titles weren't explicitly "Data Analyst." Good luck with your job search!

Yes, absolutely! A Property Specialist position at a real estate company like ALVEO can definitely involve working with data, although the extent might vary depending on the specific responsibilities and the company's data infrastructure.
Here's how a Property Specialist role could involve data:
Types of Data a Property Specialist Might Work With:
 * Property Data: Details about the properties being sold (e.g., size, number of bedrooms/bathrooms, amenities, location, pricing history, features, unique selling points).
 * Market Data: Information about the real estate market in specific areas (e.g., average selling prices, price trends, inventory levels, competitor pricing, recent sales data, neighborhood demographics, upcoming developments).
 * Customer Data: Information about potential and existing clients (e.g., contact information, preferences, budget, property viewing history, inquiries, feedback).
 * Sales Data: Records of past sales transactions (e.g., sale prices, closing dates, buyer demographics, commission earned).
 * Marketing Data: Performance of marketing campaigns (e.g., leads generated from online ads, website traffic, open house attendance).
 * Financial Data: Information related to property financing options, investment potential, and return on investment for clients.
 * Geographic Data: Information related to property locations, surrounding amenities, accessibility, and potential for future development in the area.
How a Property Specialist Might Use Data:
 * Market Research: Analyzing market data to understand trends, identify opportunities, and advise clients on optimal buying or selling strategies. For example, comparing average prices in different ALVEO developments or identifying areas with high growth potential.
 * Property Valuation: While there might be dedicated valuation teams, a Property Specialist might use comparative sales data to provide clients with an initial understanding of a property's potential value.
 * Client Relationship Management (CRM): Inputting and managing client data in CRM systems to track interactions, preferences, and follow-ups. This data can be used to personalize communication and identify potential leads.
 * Sales Reporting: Tracking their own sales performance, lead generation activities, and conversion rates. They might use basic reports or dashboards to monitor their progress.
 * Marketing and Lead Generation: Analyzing the effectiveness of different marketing channels based on the leads and sales generated.
 * Client Presentations: Using data and visualizations (e.g., charts showing price trends or investment potential) to present information to clients in a compelling way.
 * Identifying Client Needs: Analyzing client preferences and search criteria to identify suitable properties from ALVEO's portfolio.
 * Competitive Analysis: Understanding how ALVEO's properties and pricing compare to those of competitors in the market.
Leveraging Your Data Analysis Skills:
Even if the "official" job description doesn't heavily emphasize data analysis, your skills in Python, SQL, and Power BI can be a significant advantage. You could:
 * Automate Reporting: If there are repetitive reporting tasks, you could use Python scripts to automate them, saving time and improving accuracy.
 * Generate Deeper Insights: Go beyond basic reports by using SQL to query databases for more specific information and Power BI to create insightful visualizations for your own understanding or to share with your team (if appropriate). For example, analyzing which property types are most popular with certain demographics.
 * Improve Lead Generation: By analyzing marketing data, you might identify which channels are most effective in reaching potential clients for ALVEO's properties.
 * Personalize Client Interactions: By effectively managing and analyzing client data in a CRM, you can tailor your approach and property recommendations to individual client needs.
How to Document This:
Follow the same advice as before:
 * Quantify where possible: "Analyzed local market trends using Excel and identified a 10% increase in property values in [specific area] over the past year, providing key insights for client consultations."
 * Use action verbs: "Leveraged CRM data to segment leads based on property preferences, resulting in a more targeted outreach strategy."
 * Highlight tools: "Utilized Excel and potentially Power BI (if you get the chance) to create visual reports on sales performance and market comparisons."
While a Property Specialist role might not be a dedicated "Data Analyst" position, the real estate industry, including companies like ALVEO, increasingly relies on data for decision-making. Your data analysis skills can make you a more effective and valuable Property Specialist. Be proactive in identifying opportunities to use data in your role and document those instances clearly on your resume.


# Companies:
## These are list of companies that you can always check the companies page for careers hindi  na masyado pwede pag linked in lang then after applying you can find the hiring manager themself on linked in and message them directly that you've applied and that you are enthusiastic to join their team because you have X amount of skills and that you work in this manner with other people

## Tier 1:
GCash$
Maya$
BPI$
Security Bank$
ING
BDO
Philippine National Bank
Chinabank
East West Bank
Union Bank
GoTyme
Universal Robina Corps.
Accenture
IBM
PLDT
Procter & Gamble Solutions
CTBC Bank Philippines
Oracle
Manulife
Thinking Machines
Okada
Philippine Airlines
Asian Development Bank
Deutsche Bank

## Tier 2:
Cognizant
Seven Seven
Emerson
Zendesk
Stratpoint Technologies
Trend Micro
Twist Resources
VXI
Concentrix
TaskUs
Ubiquity
Telus International Philippines
Teleperformance
Media Meter
Gardenia
Sprout Solutions
SBSI
MSCI
Macquarie
Azeus Systems Ltd.
philtrustbank
development bank of the philippines

## Tier 3:
AboitizPower
Octal Philippine
Inquiro, Inc.
Medgrocer
TTEC
Integrated OS
Octal Philippines, Inc.
Lightweight Solutions
Moreton Bay Technology
HMR Philippines
Macro8 IT Solutions

For me because mas viable ang junior data analyst or junioro data scientist para sa experience ko mea ning I cannot enter into ml engineering or data engineering head first kilangan talagang yung former munan mga roles

look up company's people on linked in
if < 20 often the CEO will be interviewing and hiring for a position
if 20 - 100 often the vp of engineering or chief tech. officer
if > 100 it will be the IT director, data director or a similar director baesd on the role/field your getting into

don't try to apply for job boards and then send a cold email saying you've applied to them (kaya lang contradictory kasi other people will say after online applying you should reach out to the recruiter or hiring manager)

## Government Owned and Controlled Corporations Advantages & Disadvantages
Pros:
* If you get yourself promoted to a casual or permanent position, madami benifits. From mid-year and end of the year bonuses, uniform allowance, misc. allowances and incentives, GSIS, PhilHealth etc.
* Relaxed environment(or at least where I am working at). The job is easy, but very unfulfilling and boring. I can "destress" browsing reddit, listen to music, etc.
* Madali mag pa approve ng leaves(VL or SL) and generous sila mag bigay ng leaves. I think I already used up more than half of my 20/year VLs. You also get 5 days MANDATORY vacation leave(deducted from VL), and 3 special privilege leave(not deducted from VL or SL).
* If your boss is generous he'll give you some of that "ayuda" that he got from contractors.
* Stability. Whatever happens: typhoons, pandemic, natural disasters, etc.. (with the exception of war/invasion I guess), you'll still get your salary in full.
* Good retirement package. Monthly pensions for retirees in the gov't is based on the last 3 years' monthly salary. So let's say you managed to get 90k/month, more or less that'll be your monthly pension until you die.
* You have an opportunity to connect with or become one of the "people in high places".
* I'm handling a digitization project of our LGU and I have full autonomy regarding the project. I decide on everything from what tech stack to use up to deployment dates. What's important is it follows the relevant laws and it's IRR.
* You can take paid trainings, as well as scholarships to further your education. And afaik, this have no bond.
* You virtually have unlimited budget to use for the project at your disposal, as long as you can properly justify it to the internal budget committee and COA.
* It's not working at a government position that's a career suicide. It's being stagnant working at the same position be it government or not. You could say the same if I applied for an entry level job at a private company and stayed there for my entire career.
* Working in the government is the fastest route to get formal experience which you can use to hop to better jobs. Plus, if you are assigned on system maintenance, that opens up opportunity to upskill while being paid and earning experience for your next job. It's a win.

Cons:
* Data Analyst BUT iba ginagawa ko comapred sa job title. I'm a tech support, programmer, data encoder (you could probably automate this with python) in one. I don't do any data analytics or anything close to my job description.
* Most IT positions in government offices are dead-end jobs. Wala career growth, di ka magiging head ng IT unit.
* Most agencies/offices don't have an independent IT unit. IT Staff will be attached to the office of the agency's head. Kaya yun marami paper trail para lang maapprove IT related stuff.
* If ikaw lang programmer, wala ka matutunan sa seniors mo since glorified clerks/admin aide lang sila.
* IT personnel's equipment is on the low list of priorities. Unless laptop ng boss yan na gaming shit laptop worth 60k-150k pero pang eexcel lang syempre approved agad yan or unless may memo galing central office.
* Technologically illerate co-workers specially bosses. Dito ka maiirita, simple problem like no power sa monitor or during video cons(google meet/zoom), need nila ikaw pa gagawa ng link. Need mo pa sila bantayan during video cons and ikaw taga mute and unmute pag di mo ma unmute on time, magagalit yung boomer boss mo.
* Anything that went wrong with office IT equipment, sigurado IT Staff agad may kasalanan. Na ransomware buong office kasi yung bobong mong boss/co-worker nag open ng email and file na may ransomware? Ikaw may kasalanan.
* IT contingency planning and risk reduction doesn't exist. Our server is sitting at an old unused office room that's very dusty(yikes a big fire hazard). That's why I do my own database backups.
* You'll see how corrupt a government office is. It will drain your soul and love for country.
* I'm upskilling myself right now so I can transfer to the private sector.



-Government if you want a relaxed, okay pay, low risks, low rewards job.
-Private if you want career growth and a fulfilling career that is high in rewards, and high in risks.

recommended if you go for DICT or DOST

# Upskilling
* essentially the MS PL-300 PowerBI data analyst certification and the MS DP-300 Azure DBA certification then a AWS Cloud Practitioner certification then a AWS Solutions Architect Associate certification then finally to a AWS Data Engineer Associate certification is what will lead me to eventually become a data engineer 

and for that I need to respectively sign up for these courses in order to pass the certification exams

- Microsoft PowerBI data analyst professional certificate course
The main thing that inspired me to do this course was to learn PowerBI and learn to create beautiful dashboards with the data I have and use it as a representation of my analysis. Because in my mind I could use this learning to do a project that would help me learn further and prepare eventually for an entry level Data Analyst role, and well you know because recently I've taken a liking to analyzing data using tools like SQL and now PowerBI, an interest unbeknownst to me before had I otherwise took up the decision to try data analytics. But here I am now looking to develop my skills in creating dashboards and especially learning how to model data as I plan to use it eventually in the field of data engineering. Maybe I'm no stranger to data as of today because I've come to be acquainted with it already during days when I was fond of (although I still am) building machine learning and deep learning models. But like all new things I'm glad I took up those foundations so that I can use it to learn new things/technologies here here. Fun fact about myself is that if I wasn't in computer science or in tech I'd probably be in healthcare, but lo and behold you can use tech for healthcare so I'm quite excited to use the  skills I gain here eventually in the field of health and wellness.
~ https://learn.microsoft.com/en-us/credentials/certifications/resources/study-guides/pl-300

- a course on the dp300 azure dba certification 

this is for the phase of being a data analyst, but for the phase of now stepping into data engineering we need...

- a course on aws cloud practitioner certification
- course on aws solutions architect certification 
- course on aws data engineer certification 

### Applying for financial aid:
I am quite frankly still a student trying to build his way into having a better career in data engineering which I know will help my prospects in the future, and I firmly believe that this course on data engineering will help me so much in ways that I can use for the industry I am to endeavor in which is hopefully in this industry applied to business, or at the least an entry level role in it. I am applying for a financial aid because given my current situation I cannot at the moment being an unemployed student be able to pay for these tremendously helpful resources, and I hope I can benefit still from these resources given my situation. And I hope to repay this debt in whatever way I can given that it has helped me as I said in more ways than I can describe, and I hope that by providing me with the necessary resources to help me build my future prospects I can one day hopefully repay everything I owe that helped me get where I am now.

Currently because I'm am trying my absolute best and doing what I can to land at least an opportunity if not an entry level job in the data industry together, I firmly believe that this specific course will help me do my job intuitively in the future hopefully, which I know can give me the edge over other potential competitors and applicants in the data industry in my country. Given that I've tasted but a small aspect of the instructors at Microsoft way of teaching I genuinely believe that in learning specialization courses like this one, I can better have the skills especially the foundational skills in doing my Job more efficiently and again intuitively in the future, and moreover be a better and more productive person given the deep understanding that this course I believe offers genuinely, which I can not only use in my potential opportunities in the future but in venturing into starting my own business as well with the knowledge this has taught, teaches, and will teach me

# Commuting
* pasig to taguig (BGC)
* pasig to mandaluyong (ortigas)
* pasig to makati (corporate makati, 9 to 6 sched)
- wake up at 5, take shower, prep breakfast, commute to palengke, commute to san joaquin station for 7 oclock schedule, book destination to hulo station since malapit na sa corporate makati (unlike mrt route which would take going to palengke, then to crossing via edsa shaw jeep, then to mrt ayala station from mrt ortigas station, then another ride from mrt ayala station to the office itself which is 4 rides in total which is sayang pera)
- 
* pasig to quezon (near GMA kamuning station)
- you could take an MRT going here and walk to the office if within thhe vicinity of GMA kamuning station lang naman